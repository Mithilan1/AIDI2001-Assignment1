{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Assignment 1 - Investigating Transformer Models\n",
        "\n",
        "Course: Deep Learning for NLP\n",
        "\n",
        "This notebook is organized by the assignment rubric:\n",
        "1. How models read text (BERT tokenizer)\n",
        "2. Generating language (GPT)\n",
        "3. BERT vs GPT behavior\n",
        "4. Tokenizer/model mismatch\n",
        "5. Summarization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "from pathlib import Path\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoModelForMaskedLM,\n",
        "    AutoTokenizer,\n",
        "    pipeline,\n",
        "    set_seed,\n",
        ")\n",
        "\n",
        "random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "OUTPUT_DIR = Path(\"outputs\")\n",
        "OUTPUT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "DEVICE = 0 if torch.cuda.is_available() else -1\n",
        "print(\"CUDA available:\", torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1 - How Models Read Text (BERT tokenizer)\n",
        "\n",
        "Goal: Investigate how sentence properties (capitalization, punctuation, numbers, rare words, length) affect tokenization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bert_tok = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "sentences = [\n",
        "    \"Hello world!\",\n",
        "    \"HELLO world!\",\n",
        "    \"Hello, world!!!\",\n",
        "    \"The number 1024 is important.\",\n",
        "    \"Supercalifragilisticexpialidocious is uncommon.\",\n",
        "    \"A very very very very very long sentence with repeated words.\",\n",
        "]\n",
        "\n",
        "rows = []\n",
        "for s in sentences:\n",
        "    ids = bert_tok(s, add_special_tokens=True)[\"input_ids\"]\n",
        "    toks = bert_tok.convert_ids_to_tokens(ids)\n",
        "    rows.append({\n",
        "        \"text\": s,\n",
        "        \"char_len\": len(s),\n",
        "        \"token_len\": len(toks),\n",
        "        \"tokens\": \" \".join(toks),\n",
        "    })\n",
        "\n",
        "sentence_df = pd.DataFrame(rows)\n",
        "sentence_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(7, 4))\n",
        "plt.scatter(sentence_df[\"char_len\"], sentence_df[\"token_len\"], s=70)\n",
        "plt.xlabel(\"Character length\")\n",
        "plt.ylabel(\"BERT token count\")\n",
        "plt.title(\"Part 1: Character vs Token Length\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "sentence_df.to_csv(OUTPUT_DIR / \"part1_sentence_tokenization.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tips = sns.load_dataset(\"tips\").copy()\n",
        "\n",
        "def row_to_text(r):\n",
        "    return (\n",
        "        f\"Party size {int(r['size'])} paid total bill ${r['total_bill']:.2f} with tip ${r['tip']:.2f}. \"\n",
        "        f\"Smoker={r['smoker']}, Day={r['day']}, Time={r['time']}, Sex={r['sex']}.\"\n",
        "    )\n",
        "\n",
        "tips[\"description\"] = tips.apply(row_to_text, axis=1)\n",
        "tips[\"char_len\"] = tips[\"description\"].str.len()\n",
        "tips[\"token_len\"] = tips[\"description\"].apply(lambda x: len(bert_tok(x)[\"input_ids\"]))\n",
        "\n",
        "plt.figure(figsize=(7, 4))\n",
        "sns.histplot(tips[\"token_len\"], bins=20)\n",
        "plt.xlabel(\"BERT token count\")\n",
        "plt.ylabel(\"Rows\")\n",
        "plt.title(\"Part 1: Token length distribution for tips rows\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "tips[[\"description\", \"char_len\", \"token_len\"]].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Part 1 observations\n",
        "- Numbers and punctuation often split into separate tokens; long rare words split into subwords.\n",
        "- Character length and token length are correlated but not identical.\n",
        "- Token count varies because tokenizers encode frequent chunks efficiently and break unfamiliar strings into more pieces."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2 - Generating Language (GPT)\n",
        "\n",
        "Goal: Compare stability vs creativity by changing decoding settings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gpt_tok = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
        "gpt_model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n",
        "gpt_model.eval()\n",
        "\n",
        "if gpt_tok.pad_token_id is None:\n",
        "    gpt_tok.pad_token = gpt_tok.eos_token\n",
        "\n",
        "prompts = [\n",
        "    \"The capital of France is\",\n",
        "    \"I opened the fridge and found\",\n",
        "    \"The number 1024 is important because\",\n",
        "]\n",
        "\n",
        "sample = tips.sample(1, random_state=7).iloc[0]\n",
        "prompts.append(\n",
        "    \"Restaurant record: \"\n",
        "    f\"bill ${sample.total_bill:.2f}, tip ${sample.tip:.2f}, day {sample.day}, \"\n",
        "    f\"time {sample.time}, party size {int(sample.size)}.\"\n",
        ")\n",
        "\n",
        "def generate_many(prompt, runs, **kwargs):\n",
        "    out = []\n",
        "    for i in range(runs):\n",
        "        set_seed(100 + i)\n",
        "        ids = gpt_tok(prompt, return_tensors=\"pt\").input_ids\n",
        "        with torch.no_grad():\n",
        "            gen = gpt_model.generate(ids, **kwargs)\n",
        "        out.append(gpt_tok.decode(gen[0], skip_special_tokens=True))\n",
        "    return out\n",
        "\n",
        "greedy_cfg = dict(max_new_tokens=35, do_sample=False, pad_token_id=gpt_tok.eos_token_id)\n",
        "creative_cfg = dict(max_new_tokens=35, do_sample=True, temperature=0.9, top_p=0.9, pad_token_id=gpt_tok.eos_token_id)\n",
        "\n",
        "rows = []\n",
        "for p in prompts:\n",
        "    for i, text in enumerate(generate_many(p, 3, **greedy_cfg), start=1):\n",
        "        rows.append({\"prompt\": p, \"setting\": \"greedy\", \"run\": i, \"output\": text})\n",
        "    for i, text in enumerate(generate_many(p, 3, **creative_cfg), start=1):\n",
        "        rows.append({\"prompt\": p, \"setting\": \"creative\", \"run\": i, \"output\": text})\n",
        "\n",
        "gen_df = pd.DataFrame(rows)\n",
        "gen_df.head(12)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gen_df.to_csv(OUTPUT_DIR / \"part2_gpt_generations.csv\", index=False)\n",
        "\n",
        "comparison = (\n",
        "    gen_df.groupby([\"prompt\", \"setting\"])\n",
        "    .size()\n",
        "    .reset_index(name=\"n_outputs\")\n",
        ")\n",
        "comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Part 2 observations\n",
        "- Greedy decoding is more stable and often repeats high-probability phrasing.\n",
        "- Sampling (`temperature`, `top_p`) increases diversity and can introduce instability.\n",
        "- Creativity is controlled mainly by decoding strategy and sampling parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3 - When Models Refuse to Behave (BERT vs GPT)\n",
        "\n",
        "Test sentence: `Deep learning models are very`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = \"Deep learning models are very\"\n",
        "\n",
        "ids = gpt_tok(prompt, return_tensors=\"pt\").input_ids\n",
        "with torch.no_grad():\n",
        "    gpt_out = gpt_model.generate(\n",
        "        ids,\n",
        "        max_new_tokens=20,\n",
        "        do_sample=True,\n",
        "        temperature=0.8,\n",
        "        top_p=0.9,\n",
        "        pad_token_id=gpt_tok.eos_token_id,\n",
        "    )\n",
        "\n",
        "gpt_result = gpt_tok.decode(gpt_out[0], skip_special_tokens=True)\n",
        "\n",
        "fill_mask = pipeline(\"fill-mask\", model=\"bert-base-uncased\", tokenizer=\"bert-base-uncased\", device=DEVICE)\n",
        "masked = \"Deep learning models are very [MASK].\"\n",
        "bert_preds = fill_mask(masked, top_k=5)\n",
        "\n",
        "compare_rows = [{\n",
        "    \"model\": \"GPT (distilgpt2)\",\n",
        "    \"task\": \"continuation\",\n",
        "    \"input\": prompt,\n",
        "    \"result\": gpt_result,\n",
        "}]\n",
        "\n",
        "for pred in bert_preds:\n",
        "    compare_rows.append({\n",
        "        \"model\": \"BERT (bert-base-uncased)\",\n",
        "        \"task\": \"fill-mask\",\n",
        "        \"input\": masked,\n",
        "        \"result\": f\"{pred['token_str'].strip()} (score={pred['score']:.4f})\",\n",
        "    })\n",
        "\n",
        "compare_df = pd.DataFrame(compare_rows)\n",
        "compare_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Part 3 explanation\n",
        "- GPT succeeds at continuation because it predicts the next token left-to-right.\n",
        "- BERT fails at open-ended continuation because it is not trained as an autoregressive generator.\n",
        "- BERT works differently by predicting a masked token using both left and right context."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3.5 - Can We Swap Their Brains? (Tokenizer vs Model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mismatch_rows = []\n",
        "\n",
        "# GPT model + BERT tokenizer\n",
        "try:\n",
        "    bert_tok2 = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "    gpt_model2 = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n",
        "    ids = bert_tok2(\"The capital of France is\", return_tensors=\"pt\").input_ids\n",
        "    with torch.no_grad():\n",
        "        out = gpt_model2.generate(ids, max_new_tokens=12)\n",
        "    mismatch_rows.append({\n",
        "        \"pair\": \"GPT model + BERT tokenizer\",\n",
        "        \"status\": \"ran\",\n",
        "        \"outcome\": bert_tok2.decode(out[0], skip_special_tokens=True),\n",
        "    })\n",
        "except Exception as e:\n",
        "    mismatch_rows.append({\n",
        "        \"pair\": \"GPT model + BERT tokenizer\",\n",
        "        \"status\": \"error\",\n",
        "        \"outcome\": str(e),\n",
        "    })\n",
        "\n",
        "# BERT model + GPT tokenizer\n",
        "try:\n",
        "    gpt_tok2 = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
        "    bert_mlm = AutoModelForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
        "    ids = gpt_tok2(\"Deep learning models are very\", return_tensors=\"pt\").input_ids\n",
        "    with torch.no_grad():\n",
        "        logits = bert_mlm(ids).logits\n",
        "    pred_id = int(torch.argmax(logits[0, -1]).item())\n",
        "    mismatch_rows.append({\n",
        "        \"pair\": \"BERT model + GPT tokenizer\",\n",
        "        \"status\": \"ran\",\n",
        "        \"outcome\": f\"Predicted id {pred_id} decoded by GPT tokenizer as: {gpt_tok2.decode([pred_id])}\",\n",
        "    })\n",
        "except Exception as e:\n",
        "    mismatch_rows.append({\n",
        "        \"pair\": \"BERT model + GPT tokenizer\",\n",
        "        \"status\": \"error\",\n",
        "        \"outcome\": str(e),\n",
        "    })\n",
        "\n",
        "mismatch_df = pd.DataFrame(mismatch_rows)\n",
        "mismatch_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Final question answer\n",
        "A pretrained model generally cannot be paired with a different tokenizer because token IDs are part of the model's learned representation. If token ID 1234 meant one subword during training but maps to a different token at inference time, the embedding lookup and all downstream layers receive semantically incorrect inputs, causing degraded or nonsensical behavior."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 4 - Compressing Information (Summarization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "summarizer = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\", device=DEVICE)\n",
        "\n",
        "natural_paragraph = (\n",
        "    \"Transformer models are used for classification, question answering, and generation. \"\n",
        "    \"They perform strongly but can still hallucinate facts or repeat phrases under some decoding settings. \"\n",
        "    \"Careful evaluation is needed before deployment in high-stakes use cases.\"\n",
        ")\n",
        "\n",
        "penguins = sns.load_dataset(\"penguins\").dropna().head(8)\n",
        "structured_text = \" \".join([\n",
        "    f\"Penguin {i}: species {r.species}, island {r.island}, bill length {r.bill_length_mm:.1f} mm, \"\n",
        "    f\"bill depth {r.bill_depth_mm:.1f} mm, flipper {r.flipper_length_mm:.1f} mm, mass {r.body_mass_g:.1f} g.\"\n",
        "    for i, r in penguins.iterrows()\n",
        "])\n",
        "\n",
        "mixed_text = (\n",
        "    \"Field notes from a biology class: students measured penguins and discussed species differences. \"\n",
        "    + structured_text +\n",
        "    \" The instructor asked students to interpret size patterns across islands.\"\n",
        ")\n",
        "\n",
        "inputs = {\n",
        "    \"natural_paragraph\": natural_paragraph,\n",
        "    \"structured_penguins\": structured_text,\n",
        "    \"mixed_structured_natural\": mixed_text,\n",
        "}\n",
        "\n",
        "sum_rows = []\n",
        "for name, text in inputs.items():\n",
        "    out = summarizer(text, max_length=75, min_length=20, do_sample=False)[0][\"summary_text\"]\n",
        "    sum_rows.append({\"input_name\": name, \"summary\": out})\n",
        "\n",
        "sum_df = pd.DataFrame(sum_rows)\n",
        "sum_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Part 4 observations\n",
        "- One summary that works well: `natural_paragraph` usually preserves the main message.\n",
        "- One summary that loses important information: `structured_penguins` often drops numeric detail.\n",
        "- Consistent mistake: model compresses many measurements into vague language.\n",
        "- Structured data is difficult because summarization models are mainly trained on narrative text, not dense tabular facts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Submission checklist\n",
        "- Experiments included for all required parts.\n",
        "- Tables/figures included for tokenization and generation comparison.\n",
        "- Short explanations provided under each section.\n",
        "\n",
        "Run all cells before submission and verify outputs are visible."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}